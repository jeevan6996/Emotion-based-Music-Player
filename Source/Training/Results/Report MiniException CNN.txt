(ImageFace) jeevan@hp-pavilion-15:~/Documents/BE Project/Final Dataset+Model$ python3 trainModel.py
Using TensorFlow backend.
[INFO] loading images...
(19070, 64, 64, 1)
(19070,)
[INFO] training network...
2019-04-02 21:25:45.698694: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 64, 64, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 62, 62, 8)    72          input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 62, 62, 8)    32          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 62, 62, 8)    0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 60, 60, 8)    576         activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 60, 60, 8)    32          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 60, 60, 8)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 60, 60, 16)   200         activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 60, 60, 16)   64          separable_conv2d_1[0][0]         
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 60, 60, 16)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 60, 60, 16)   400         activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 60, 60, 16)   64          separable_conv2d_2[0][0]         
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 30, 30, 16)   128         activation_2[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 30, 30, 16)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 30, 30, 16)   64          conv2d_3[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 30, 30, 16)   0           max_pooling2d_1[0][0]            
                                                                 batch_normalization_3[0][0]      
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 30, 30, 32)   656         add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 30, 30, 32)   128         separable_conv2d_3[0][0]         
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 30, 30, 32)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 30, 30, 32)   1312        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 30, 30, 32)   128         separable_conv2d_4[0][0]         
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 15, 15, 32)   512         add_1[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 15, 15, 32)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 15, 15, 32)   128         conv2d_4[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 15, 15, 32)   0           max_pooling2d_2[0][0]            
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 15, 15, 64)   2336        add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 15, 15, 64)   256         separable_conv2d_5[0][0]         
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 15, 15, 64)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 15, 15, 64)   4672        activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 15, 15, 64)   256         separable_conv2d_6[0][0]         
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 8, 8, 64)     2048        add_2[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 8, 8, 64)     256         conv2d_5[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 8, 8, 64)     0           max_pooling2d_3[0][0]            
                                                                 batch_normalization_9[0][0]      
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 8, 8, 128)    8768        add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_7[0][0]         
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 8, 8, 128)    0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 8, 8, 128)    17536       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_8[0][0]         
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 4, 4, 128)    8192        add_3[0][0]                      
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 4, 4, 128)    512         conv2d_6[0][0]                   
__________________________________________________________________________________________________
add_4 (Add)                     (None, 4, 4, 128)    0           max_pooling2d_4[0][0]            
                                                                 batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 4, 4, 4)      4612        add_4[0][0]                      
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 4)            0           conv2d_7[0][0]                   
__________________________________________________________________________________________________
predictions (Activation)        (None, 4)            0           global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 54,964
Trainable params: 53,492
Non-trainable params: 1,472
__________________________________________________________________________________________________
Train on 14302 samples, validate on 4768 samples
Epoch 1/200
2019-04-02 21:26:02.483070: W tensorflow/core/framework/allocator.cc:122] Allocation of 25141248 exceeds 10% of system memory.
2019-04-02 21:26:02.483070: W tensorflow/core/framework/allocator.cc:122] Allocation of 25141248 exceeds 10% of system memory.
2019-04-02 21:26:02.958101: W tensorflow/core/framework/allocator.cc:122] Allocation of 29030400 exceeds 10% of system memory.
2019-04-02 21:26:03.428932: W tensorflow/core/framework/allocator.cc:122] Allocation of 16744464 exceeds 10% of system memory.
   32/14302 [..............................] - ETA: 1:20:34 - loss: 1.5267 - acc: 0.40622019-04-02 21:26:04.484171: W tensorflow/core/framework/allocator.cc:122] Allocation of 25141248 exceeds 10% of system memory.
14302/14302 [==============================] - 298s 21ms/step - loss: 1.3541 - acc: 0.4105 - val_loss: 1.4547 - val_acc: 0.3725
Epoch 2/200
14302/14302 [==============================] - 295s 21ms/step - loss: 1.0948 - acc: 0.5427 - val_loss: 1.2999 - val_acc: 0.4551
Epoch 3/200
14302/14302 [==============================] - 344s 24ms/step - loss: 0.9741 - acc: 0.6026 - val_loss: 1.0930 - val_acc: 0.5464
Epoch 4/200
14302/14302 [==============================] - 270s 19ms/step - loss: 0.8938 - acc: 0.6371 - val_loss: 0.9480 - val_acc: 0.6166
Epoch 5/200
14302/14302 [==============================] - 284s 20ms/step - loss: 0.8399 - acc: 0.6653 - val_loss: 1.1829 - val_acc: 0.5470
Epoch 6/200
14302/14302 [==============================] - 657s 46ms/step - loss: 0.7740 - acc: 0.6912 - val_loss: 1.2291 - val_acc: 0.5193
Epoch 7/200
14302/14302 [==============================] - 270s 19ms/step - loss: 0.7338 - acc: 0.7130 - val_loss: 1.1314 - val_acc: 0.5828
Epoch 8/80
14302/14302 [==============================] - 271s 14ms/step - loss: 0.5273 - acc: 0.8192 - val_loss: 0.7172 - val_acc: 0.6909
Epoch 9/80
14302/14302 [==============================] - 275s 15ms/step - loss: 0.4808 - acc: 0.8166 - val_loss: 0.4369 - val_acc: 0.7024
Epoch 10/80
14302/14302 [==============================] - 272s 14ms/step - loss: 0.3825 - acc: 0.8713 - val_loss: 0.3435 - val_acc: 0.7588
Epoch 11/80
14302/14302 [==============================] - 278s 14ms/step - loss: 0.3489 - acc: 0.8622 - val_loss: 0.7296 - val_acc: 0.7155
Epoch 12/80
14302/14302 [==============================] - 270s 14ms/step - loss: 0.3703 - acc: 0.8687 - val_loss: 0.3585 - val_acc: 0.7936
Epoch 13/80
14302/14302 [==============================] - 289s 14ms/step - loss: 0.2832 - acc: 0.8869 - val_loss: 1.8214 - val_acc: 0.7842
Epoch 14/80
14302/14302 [==============================] - 236s 15ms/step - loss: 0.4069 - acc: 0.8661 - val_loss: 0.6053 - val_acc: 0.8100
Epoch 15/80
14302/14302 [==============================] - 212s 15ms/step - loss: 0.2316 - acc: 0.9103 - val_loss: 0.2045 - val_acc: 0.8373
Epoch 16/80
14302/14302 [==============================] - 210s 14ms/step - loss: 0.1491 - acc: 0.9454 - val_loss: 0.4182 - val_acc: 0.8094
Epoch 17/80
14302/14302 [==============================] - 220s 13ms/step - loss: 0.1320 - acc: 0.9610 - val_loss: 0.1694 - val_acc: 0.7755
Epoch 18/80
14302/14302 [==============================] - 262s 15ms/step - loss: 0.1915 - acc: 0.9350 - val_loss: 0.5602 - val_acc: 0.7970
Epoch 19/80
Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
14302/14302 [==============================] - 272s 21ms/step - loss: 0.2971 - acc: 0.8956 - val_loss: 0.3904 - val_acc: 0.7547
Epoch 20/200
14302/14302 [==============================] - 276s 19ms/step - loss: 0.1720 - acc: 0.9564 - val_loss: 0.1197 - val_acc: 0.7854
Epoch 21/200
14302/14302 [==============================] - 272s 19ms/step - loss: 0.1339 - acc: 0.9756 - val_loss: 0.1292 - val_acc: 0.8108
Epoch 22/200
14302/14302 [==============================] - 271s 19ms/step - loss: 0.1180 - acc: 0.9810 - val_loss: 0.1299 - val_acc: 0.8292
Epoch 23/200
14302/14302 [==============================] - 273s 19ms/step - loss: 0.1078 - acc: 0.9856 - val_loss: 0.1531 - val_acc: 0.8088
Epoch 24/200
14302/14302 [==============================] - 272s 19ms/step - loss: 0.0991 - acc: 0.9888 - val_loss: 1.2087 - val_acc: 0.8216
Epoch 25/200
14302/14302 [==============================] - 272s 19ms/step - loss: 0.0916 - acc: 0.9908 - val_loss: 0.1756 - val_acc: 0.8362
Epoch 26/200
14302/14302 [==============================] - 297s 21ms/step - loss: 0.0866 - acc: 0.9914 - val_loss: 0.1289 - val_acc: 0.8442
Epoch 27/200
14302/14302 [==============================] - 275s 19ms/step - loss: 0.0788 - acc: 0.9937 - val_loss: 0.1237 - val_acc: 0.8516
Epoch 28/200
14302/14302 [==============================] - 270s 19ms/step - loss: 0.0750 - acc: 0.9943 - val_loss: 0.1503 - val_acc: 0.82544
Epoch 29/200
14302/14302 [==============================] - 269s 19ms/step - loss: 0.0706 - acc: 0.9947 - val_loss: 0.1997 - val_acc: 0.8521
Epoch 30/200
14302/14302 [==============================] - 267s 19ms/step - loss: 0.0653 - acc: 0.9966 - val_loss: 0.1959 - val_acc: 0.8408
Epoch 31/200
14302/14302 [==============================] - 267s 19ms/step - loss: 0.0614 - acc: 0.9966 - val_loss: 0.1920 - val_acc: 0.8629
Epoch 32/200
14302/14302 [==============================] - 270s 19ms/step - loss: 0.0586 - acc: 0.9973 - val_loss: 0.1365 - val_acc: 0.8893
Epoch 33/200
14302/14302 [==============================] - 269s 19ms/step - loss: 0.0566 - acc: 0.9972 - val_loss: 0.1457 - val_acc: 0.8914
Epoch 34/200
14302/14302 [==============================] - 269s 19ms/step - loss: 0.0529 - acc: 0.9972 - val_loss: 0.1770 - val_acc: 0.8777

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 35/200
14302/14302 [==============================] - 269s 19ms/step - loss: 0.0471 - acc: 0.9980 - val_loss: 0.1209 - val_acc: 0.9071
Epoch 36/200
14302/14302 [==============================] - 853s 60ms/step - loss: 0.0463 - acc: 0.9985 - val_loss: 0.1447 - val_acc: 0.8642
Epoch 37/200
   64/14302 [..............................] - ETA: 8:16:10 - loss: 0.0385 - acc: 1.0000 /home/jeevan/.local/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.265395). Check your callbacks.
  % delta_t_median)
14302/14302 [==============================] - 481s 34ms/step - loss: 0.0442 - acc: 0.9993 - val_loss: 0.1442 - val_acc: 0.8535
Epoch 38/200
14302/14302 [==============================] - 280s 20ms/step - loss: 0.0443 - acc: 0.9985 - val_loss: 0.1499 - val_acc: 0.8750
Epoch 39/200
14302/14302 [==============================] - 267s 19ms/step - loss: 0.0439 - acc: 0.9989 - val_loss: 0.1512 - val_acc: 0.8769
Epoch 40/200
14302/14302 [==============================] - 266s 19ms/step - loss: 0.0438 - acc: 0.9992 - val_loss: 0.1530 - val_acc: 0.8844
Epoch 41/200
14302/14302 [==============================] - 267s 19ms/step - loss: 0.0430 - acc: 0.9988 - val_loss: 0.1561 - val_acc: 0.8667
Epoch 42/200
14302/14302 [==============================] - 266s 19ms/step - loss: 0.0426 - acc: 0.9988 - val_loss: 0.1599 - val_acc: 0.8748
Epoch 43/200
14302/14302 [==============================] - 265s 19ms/step - loss: 0.0421 - acc: 0.9987 - val_loss: 0.1608 - val_acc: 0.8750
Epoch 44/200
14302/14302 [==============================] - 265s 19ms/step - loss: 0.0422 - acc: 0.9984 - val_loss: 0.1650 - val_acc: 0.8848
[INFO] evaluating network...
              precision    recall  f1-score   support

       Angry       0.82      0.72      0.77      1222
       Happy       1.00      1.00      1.00      1144
     Neutral       0.89      0.81      0.85      1210
         Sad       0.78      0.63      0.70      1192

   micro avg       0.87      0.87      0.87      4768
   macro avg       0.87      0.87      0.87      4768
weighted avg       0.87      0.87      0.87      4768

[INFO] serializing network and label binarizer...
FINISHED.
