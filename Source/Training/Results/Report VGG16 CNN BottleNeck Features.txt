(ImageFace) jeevan@hp-pavilion-15:~/Documents/BE Project/Database Augmentation$ python3  featExtract.py 
Using TensorFlow backend.
[INFO] Loading images from Dataset...
(19070, 64, 64, 1)
(19070,)
[INFO] training network...
2019-03-27 11:33:01.720626: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
Train on 14302 samples, validate on 4768 samples
Epoch 1/80
14302/14302 [==============================] - 12s 15ms/step - loss: 1.3471 - acc: 0.2757 - val_loss: 4.2911 - val_acc: 0.4485
Epoch 2/80
14302/14302 [==============================] - 11s 14ms/step - loss: 1.2345 - acc: 0.4226 - val_loss: 4.2683 - val_acc: 0.3788
Epoch 3/80
14302/14302 [==============================] - 11s 14ms/step - loss: 1.0469 - acc: 0.5878 - val_loss: 4.0685 - val_acc: 0.4485
Epoch 4/80
14302/14302 [==============================] - 10s 14ms/step - loss: 0.9299 - acc: 0.5943 - val_loss: 4.9445 - val_acc: 0.5788
Epoch 5/80
14302/14302 [==============================] - 10s 13ms/step - loss: 0.9037 - acc: 0.6268 - val_loss: 4.8630 - val_acc: 0.6364
Epoch 6/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.6717 - acc: 0.7685 - val_loss: 4.6193 - val_acc: 0.5879
Epoch 7/80
14302/14302 [==============================] - 10s 13ms/step - loss: 0.6330 - acc: 0.7646 - val_loss: 4.7451 - val_acc: 0.5994
Epoch 8/200
14302/14302 [==============================] - 11s 19ms/step - loss: 0.6946 - acc: 0.7279 - val_loss: 4.0599 - val_acc: 0.5889
Epoch 9/200
14302/14302 [==============================] - 11s 19ms/step - loss: 0.6518 - acc: 0.7452 - val_loss: 3.2897 - val_acc: 0.5994
Epoch 10/200
14302/14302 [==============================] - 10s 19ms/step - loss: 0.6018 - acc: 0.7675 - val_loss: 3.1454 - val_acc: 0.5682
Epoch 11/200
14302/14302 [==============================] - 11s 19ms/step - loss: 0.5641 - acc: 0.7846 - val_loss: 3.0697 - val_acc: 0.6049
Epoch 12/200
14302/14302 [==============================] - 10s 19ms/step - loss: 0.5312 - acc: 0.7973 - val_loss: 3.3843 - val_acc: 0.5508
Epoch 13/200
14302/14302 [==============================] - 11s 19ms/step - loss: 0.4874 - acc: 0.8195 - val_loss: 2.1444 - val_acc: 0.6116
Epoch 14/200
14302/14302 [==============================] - 12s 19ms/step - loss: 0.4480 - acc: 0.8344 - val_loss: 3.6595 - val_acc: 0.5581
Epoch 15/200
14302/14302 [==============================] - 10s 19ms/step - loss: 0.4212 - acc: 0.8414 - val_loss: 2.1682 - val_acc: 0.6120
Epoch 16/200
14302/14302 [==============================] - 11s 19ms/step - loss: 0.3828 - acc: 0.8584 - val_loss: 3.2720 - val_acc: 0.5925
Epoch 17/200
14302/14302 [==============================] - 11s 19ms/step - loss: 0.3551 - acc: 0.8715 - val_loss: 3.6116 - val_acc: 0.5480
Epoch 18/200
14302/14302 [==============================] - 10s 19ms/step - loss: 0.3216 - acc: 0.8858 - val_loss: 3.5694 - val_acc: 0.5784
Epoch 19/200
14302/14302 [==============================] - 10s 21ms/step - loss: 0.2971 - acc: 0.8956 - val_loss: 3.3904 - val_acc: 0.6047
Epoch 20/80
14302/14302 [==============================] - 10s 13ms/step - loss: 0.0656 - acc: 0.9779 - val_loss: 2.1568 - val_acc: 0.6636
Epoch 21/80
14302/14302 [==============================] - 10s 13ms/step - loss: 0.3427 - acc: 0.8947 - val_loss: 2.4347 - val_acc: 0.6273
Epoch 22/80
14302/14302 [==============================] - 10s 13ms/step - loss: 0.0852 - acc: 0.9792 - val_loss: 2.1956 - val_acc: 0.6542
Epoch 23/80
14302/14302 [==============================] - 10s 13ms/step - loss: 0.1638 - acc: 0.9480 - val_loss: 3.3476 - val_acc: 0.5567
Epoch 24/80
14302/14302 [==============================] - 10s 13ms/step - loss: 0.1269 - acc: 0.9532 - val_loss: 3.2485 - val_acc: 0.5761
Epoch 25/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.1019 - acc: 0.9597 - val_loss: 2.1472 - val_acc: 0.6085
Epoch 26/80
14302/14302 [==============================] - 11s 15ms/step - loss: 0.0482 - acc: 0.9896 - val_loss: 2.1851 - val_acc: 0.6542
Epoch 27/80
14302/14302 [==============================] - 12s 16ms/step - loss: 0.0548 - acc: 0.9818 - val_loss: 2.1503 - val_acc: 0.6703
Epoch 28/80
14302/14302 [==============================] - 12s 15ms/step - loss: 0.0308 - acc: 0.9909 - val_loss: 2.1938 - val_acc: 0.6264
Epoch 29/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0277 - acc: 0.9974 - val_loss: 1.1423 - val_acc: 0.6936
Epoch 30/80
14302/14302 [==============================] - 13s 17ms/step - loss: 0.0156 - acc: 0.9974 - val_loss: 2.1287 - val_acc: 0.6506
Epoch 31/80
14302/14302 [==============================] - 12s 15ms/step - loss: 0.0121 - acc: 0.9987 - val_loss: 1.1467 - val_acc: 0.7145
Epoch 32/80
14302/14302 [==============================] - 10s 14ms/step - loss: 0.0224 - acc: 0.9948 - val_loss: 2.6364 - val_acc: 0.6394
Epoch 33/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0359 - acc: 0.9870 - val_loss: 1.1342 - val_acc: 0.7024
Epoch 34/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0124 - acc: 0.9974 - val_loss: 2.1466 - val_acc: 0.6576
Epoch 35/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0170 - acc: 0.9974 - val_loss: 3.1375 - val_acc: 0.6485
Epoch 36/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0092 - acc: 0.9987 - val_loss: 2.5380 - val_acc: 0.6545
Epoch 37/80
14302/14302 [==============================] - 11s 15ms/step - loss: 0.0238 - acc: 0.9909 - val_loss: 2.1412 - val_acc: 0.6515
Epoch 38/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0114 - acc: 0.9987 - val_loss: 2.4593 - val_acc: 0.6788
Epoch 39/80
14302/14302 [==============================] - 12s 15ms/step - loss: 0.0273 - acc: 0.9883 - val_loss: 2.1238 - val_acc: 0.6636
Epoch 40/80
14302/14302 [==============================] - 13s 16ms/step - loss: 0.0078 - acc: 0.9987 - val_loss: 2.1361 - val_acc: 0.6667
Epoch 41/80
14302/14302 [==============================] - 14s 18ms/step - loss: 0.0586 - acc: 0.9792 - val_loss: 1.2795 - val_acc: 0.7152
Epoch 42/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.4969 - acc: 0.8674 - val_loss: 2.3682 - val_acc: 0.6424
Epoch 43/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0395 - acc: 0.9948 - val_loss: 1.1321 - val_acc: 0.7545
Epoch 44/80
14302/14302 [==============================] - 10s 14ms/step - loss: 0.0203 - acc: 0.9961 - val_loss: 1.1270 - val_acc: 0.7606
Epoch 45/80
14302/14302 [==============================] - 10s 14ms/step - loss: 0.0238 - acc: 0.9948 - val_loss: 3.4848 - val_acc: 0.6364
Epoch 46/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.3400 - acc: 0.9038 - val_loss: 2.1433 - val_acc: 0.6515
Epoch 47/80
14302/14302 [==============================] - 10s 14ms/step - loss: 0.0239 - acc: 0.9974 - val_loss: 2.1523 - val_acc: 0.6667
Epoch 48/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0185 - acc: 0.9961 - val_loss: 2.1197 - val_acc: 0.6576
Epoch 49/80
14302/14302 [==============================] - 12s 15ms/step - loss: 0.0111 - acc: 0.9974 - val_loss: 2.1446 - val_acc: 0.6455
Epoch 50/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0106 - acc: 0.9974 - val_loss: 2.1209 - val_acc: 0.6697
Epoch 51/80
14302/14302 [==============================] - 12s 16ms/step - loss: 0.0115 - acc: 0.9974 - val_loss: 3.1351 - val_acc: 0.6485
Epoch 52/80
14302/14302 [==============================] - 11s 15ms/step - loss: 0.0121 - acc: 0.9987 - val_loss: 2.1182 - val_acc: 0.6636
Epoch 53/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0081 - acc: 0.9974 - val_loss: 2.1115 - val_acc: 0.6697
Epoch 54/80
14302/14302 [==============================] - 12s 15ms/step - loss: 0.0081 - acc: 0.9974 - val_loss: 2.2063 - val_acc: 0.6364
Epoch 55/80
14302/14302 [==============================] - 11s 15ms/step - loss: 0.0559 - acc: 0.9831 - val_loss: 2.1323 - val_acc: 0.6606
Epoch 56/80
14302/14302 [==============================] - 11s 15ms/step - loss: 0.0087 - acc: 0.9974 - val_loss: 2.1252 - val_acc: 0.6727
Epoch 57/80
14302/14302 [==============================] - 11s 15ms/step - loss: 0.0072 - acc: 0.9987 - val_loss: 2.1362 - val_acc: 0.6667
Epoch 58/80
14302/14302 [==============================] - 10s 14ms/step - loss: 0.0083 - acc: 0.9974 - val_loss: 2.2762 - val_acc: 0.6152
Epoch 59/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0096 - acc: 0.9961 - val_loss: 2.1192 - val_acc: 0.6727
Epoch 60/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 2.1136 - val_acc: 0.6697
Epoch 61/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 1.1276 - val_acc: 0.7636
Epoch 62/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0087 - acc: 0.9974 - val_loss: 2.1116 - val_acc: 0.6727
Epoch 63/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0058 - acc: 0.9987 - val_loss: 3.9248 - val_acc: 0.6909
Epoch 64/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.1512 - acc: 0.9740 - val_loss: 2.1178 - val_acc: 0.6606
Epoch 65/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0063 - acc: 0.9987 - val_loss: 3.1162 - val_acc: 0.6606
Epoch 66/80
14302/14302 [==============================] - 11s 15ms/step - loss: 0.0070 - acc: 0.9974 - val_loss: 3.1345 - val_acc: 0.6636
Epoch 67/80
14302/14302 [==============================] - 11s 15ms/step - loss: 0.0101 - acc: 0.9987 - val_loss: 2.1320 - val_acc: 0.6236
Epoch 68/80
14302/14302 [==============================] - 10s 14ms/step - loss: 0.0095 - acc: 0.9961 - val_loss: 2.1401 - val_acc: 0.6776
Epoch 69/80
14302/14302 [==============================] - 10s 14ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 2.1198 - val_acc: 0.6636
Epoch 70/80
14302/14302 [==============================] - 10s 13ms/step - loss: 0.0069 - acc: 0.9974 - val_loss: 3.1342 - val_acc: 0.5936
Epoch 71/80
14302/14302 [==============================] - 10s 14ms/step - loss: 0.0066 - acc: 0.9974 - val_loss: 2.1208 - val_acc: 0.6036
Epoch 72/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0048 - acc: 0.9974 - val_loss: 2.1254 - val_acc: 0.6697
Epoch 73/80
14302/14302 [==============================] - 12s 15ms/step - loss: 0.0137 - acc: 0.9961 - val_loss: 2.3063 - val_acc: 0.6182
Epoch 74/80
14302/14302 [==============================] - 12s 16ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 2.1180 - val_acc: 0.6667
Epoch 75/80
14302/14302 [==============================] - 12s 15ms/step - loss: 0.0034 - acc: 0.9987 - val_loss: 2.1244 - val_acc: 0.6667
Epoch 76/80
14302/14302 [==============================] - 12s 16ms/step - loss: 0.0042 - acc: 0.9974 - val_loss: 2.1928 - val_acc: 0.6785
Epoch 77/80
14302/14302 [==============================] - 13s 17ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 1.1520 - val_acc: 0.6976
Epoch 78/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0093 - acc: 0.9974 - val_loss: 1.1584 - val_acc: 0.7255
Epoch 79/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0069 - acc: 0.9974 - val_loss: 1.1272 - val_acc: 0.7136
Epoch 80/80
14302/14302 [==============================] - 11s 14ms/step - loss: 0.0029 - acc: 0.9987 - val_loss: 2.2311 - val_acc: 0.6767
[INFO] evaluating network...
              precision    recall  f1-score   support

       Angry       0.62      0.58      0.60      1222
       Happy       0.78      0.65      0.70      1144
     Neutral       0.65      0.51      0.57      1210
         Sad       0.53      0.48      0.50      1192

   micro avg       0.64      0.64      0.64      4768
   macro avg       0.64      0.64      0.64      4768
weighted avg       0.64      0.64      0.64      4768

[INFO] serializing network and label binarizer...
FINISHED.
